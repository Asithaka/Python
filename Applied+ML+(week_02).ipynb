{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 1 - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 15\n",
    "x = np.linspace(0,10,n) + np.random.randn(n)/5\n",
    "y = np.sin(x)+x/6 + np.random.randn(n)/10\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question 1\n",
    "# Write a function that fits a polynomial LinearRegression model on the training data X_train for degrees 1, 3, 6, and 9.\n",
    "# (Use PolynomialFeatures in sklearn.preprocessing to create the polynomial features and then fit a linear regression model) \n",
    "# For each model, find 100 predicted values over the interval x = 0 to 10 (e.g. np.linspace(0,10,100)) and store this in \n",
    "# a numpy array. The first row of this array should correspond to the output from the model trained on degree 1, the second \n",
    "# row degree 3, the third row degree 6, and the fourth row degree 9.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    predicted_values=np.zeros((4,100))\n",
    "    \n",
    "    n=[1,3,6,9]\n",
    "    \n",
    "    for count,degree in enumerate(n,0):\n",
    "        \n",
    "        polynom=PolynomialFeatures(degree=degree)\n",
    "        \n",
    "        X_polynom=polynom.fit_transform(X_train.reshape(-1,1))\n",
    "        \n",
    "        polyReg=LinearRegression()\n",
    "        \n",
    "        polyReg.fit(X_polynom,y_train)\n",
    "        \n",
    "        y_predict=polyReg.predict(polynom.fit_transform(np.linspace(0,10,100).reshape(-1,1)))\n",
    "        \n",
    "        predicted_values[count,:]=y_predict\n",
    "    \n",
    "    return predicted_values \n",
    "\n",
    "print(answer_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 2\n",
    "# Write a function that fits a polynomial LinearRegression model on the training data X_train for degrees 0 through 9. \n",
    "# For each model compute the  R2R2  (coefficient of determination) regression score on the training data as well as the \n",
    "# the test data, and return both of these arrays in a tuple.\n",
    "\n",
    "# This function should return one tuple of numpy arrays (r2_train, r2_test). Both arrays should have shape (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_two():\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.metrics.regression import r2_score\n",
    "    \n",
    "    R2_test=np.zeros(10)\n",
    "    \n",
    "    R2_train=np.zeros(10)\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        polynom_1=PolynomialFeatures(degree=i)\n",
    "        \n",
    "        X_polynom_1=polynom_1.fit_transform(X_train.reshape(-1,1))\n",
    "        \n",
    "        X_test_polynom_1=polynom_1.fit_transform(X_test.reshape(-1,1))\n",
    "        \n",
    "        PolyReg_1=LinearRegression().fit(X_polynom_1,y_train)\n",
    "        \n",
    "        R2_train[i]=PolyReg_1.score( X_polynom_1,y_train)\n",
    "        \n",
    "        \n",
    "        R2_test[i]=PolyReg_1.score(X_test_polynom_1,y_test)\n",
    "        \n",
    "    return (R2_train,R2_test)\n",
    "\n",
    "print(answer_two())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 3\n",
    "# Based on the  R2R2  scores from question 2 (degree levels 0 through 9), what degree level corresponds to a model that is \n",
    "# underfitting? What degree level corresponds to a model that is overfitting? What choice of degree level would provide a \n",
    "# model with good generalization performance on this dataset?\n",
    "\n",
    "# Hint: Try plotting the  R2R2  scores from question 2 to visualize the relationship between degree level and  R2R2 . Remember \n",
    "# to comment out the import matplotlib line before submission.\n",
    "\n",
    "# This function should return one tuple with the degree values in this order: (Underfitting, Overfitting, Good_Generalization). \n",
    "# There might be multiple correct solutions, however, you only need to return one possible solution, for example, (1,2,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    \n",
    "    R2_Score=answer_two()\n",
    "    \n",
    "    data={'R2_train':R2_Score[0],'R2_test':R2_Score[1]}\n",
    "\n",
    "    df=pd.DataFrame(data)\n",
    "\n",
    "    df['Score_diff']=df['R2_train']-df['R2_test']\n",
    "\n",
    "    df_2=df.sort_values(by=['R2_train'])\n",
    "\n",
    "    Underfitting=df_2.index[0]\n",
    "\n",
    "    df_3=df.sort_values(by=['Score_diff'])\n",
    "\n",
    "    Overfitting=df_3.index[-1]\n",
    "\n",
    "    Good_Generalization=df_3.index[0]\n",
    "    \n",
    "    return (Underfitting, Overfitting, Good_Generalization)\n",
    "\n",
    "print(answer_three())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 4\n",
    "# Training models on high degree polynomial features can result in overly complex models that overfit, so we often use \n",
    "# regularized versions of the model to constrain model complexity, as we saw with Ridge and Lasso linear regression.\n",
    "\n",
    "# For this question, train two models: a non-regularized LinearRegression model (default parameters) and a regularized \n",
    "# Lasso Regression model (with parameters alpha=0.01, max_iter=10000) both on polynomial features of degree 12. Return the  \n",
    "# R2R2  score for both the LinearRegression and Lasso model's test sets.\n",
    "\n",
    "# This function should return one tuple (LinearRegression_R2_test_score, Lasso_R2_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.linear_model import Lasso, LinearRegression\n",
    "    from sklearn.metrics.regression import r2_score\n",
    "\n",
    "    poly_feature = PolynomialFeatures(degree=12)\n",
    "    \n",
    "    X_train_5 = poly_feature .fit_transform(X_train.reshape(-1,1))\n",
    "    \n",
    "    X_test_5 = poly_feature .fit_transform(X_test.reshape(-1,1))\n",
    "    \n",
    "    lineReg=LinearRegression().fit(X_train_5,y_train)\n",
    "\n",
    "    LinearRegression_R2_test_score=lineReg.score(X_test_5, y_test)\n",
    "\n",
    "    linelasso=Lasso(alpha=0.01,max_iter=10000).fit(X_train_5,y_train)\n",
    "\n",
    "    Lasso_R2_test_score=linelasso.score(X_test_5, y_test)\n",
    "\n",
    "    return (LinearRegression_R2_test_score,Lasso_R2_test_score)\n",
    "\n",
    "print(answer_four())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Part 2 - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mush_df = pd.read_csv('mushrooms.csv')\n",
    "\n",
    "mush_df2 = pd.get_dummies(mush_df)\n",
    "\n",
    "X_mush = mush_df2.iloc[:,2:]\n",
    "y_mush = mush_df2.iloc[:,1]\n",
    "\n",
    "# use the variables X_train2, y_train2 for Question 5\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_mush, y_mush, random_state=0)\n",
    "\n",
    "# For performance reasons in Questions 6 and 7, we will create a smaller version of the\n",
    "# entire mushroom dataset for use in those questions.  For simplicity we'll just re-use\n",
    "# the 25% test split created above as the representative subset.\n",
    "#\n",
    "# Use the variables X_subset, y_subset for Questions 6 and 7.\n",
    "X_subset = X_test2\n",
    "y_subset = y_test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 5\n",
    "# Using X_train2 and y_train2 from the preceeding cell, train a DecisionTreeClassifier with default parameters and \n",
    "# random_state=0. What are the 5 most important features found by the decision tree?\n",
    "\n",
    "# As a reminder, the feature names are available in the X_train2.columns property, and the order of the features \n",
    "# in X_train2.columns matches the order of the feature importance values in the classifier's feature_importances_ property.\n",
    "\n",
    "# This function should return a list of length 5 containing the feature names in descending order of importance.\n",
    "\n",
    "# Note: remember that you also need to set random_state in the DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    clf=DecisionTreeClassifier(random_state = 0).fit(X_train2,y_train2)\n",
    "\n",
    "    x=pd.DataFrame({'features':clf.feature_importances_,'Names':X_train2.columns}) \n",
    "\n",
    "    x.sort_values(by=['features'],inplace=True, ascending=False)\n",
    "\n",
    "    x=x.reset_index(drop=True)\n",
    "    \n",
    "    return list(x.iloc[0:5,0])\n",
    "\n",
    "print(answer_five())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 6\n",
    "# For this question, we're going to use the validation_curve function in sklearn.model_selection to determine training and \n",
    "# test scores for a Support Vector Classifier (SVC) with varying parameter values. Recall that the validation_curve function, \n",
    "# in addition to taking an initialized unfitted classifier object, takes a dataset as input and does its own internal \n",
    "# train-test splits to compute results.\n",
    "\n",
    "# Because creating a validation curve requires fitting multiple models, for performance reasons this question will use just \n",
    "# a subset of the original mushroom dataset: please use the variables X_subset and y_subset as input to the validation curve \n",
    "# function (instead of X_mush and y_mush) to reduce computation time.\n",
    "\n",
    "# The initialized unfitted classifier object we'll be using is a Support Vector Classifier with radial basis kernel. \n",
    "# So your first step is to create an SVC object with default parameters (i.e. kernel='rbf', C=1) and random_state=0. \n",
    "# Recall that the kernel width of the RBF kernel is controlled using the gamma parameter.\n",
    "\n",
    "# With this classifier, and the dataset in X_subset, y_subset, explore the effect of gamma on classifier accuracy by using\n",
    "# the validation_curve function to find the training and test scores for 6 values of gamma from 0.0001 to 10 \n",
    "# (i.e. np.logspace(-4,1,6)). Recall that you can specify what scoring metric you want validation_curve to use by setting \n",
    "# the \"scoring\" parameter. In this case, we want to use \"accuracy\" as the scoring metric.\n",
    "\n",
    "# For each level of gamma, validation_curve will fit 3 models on different subsets of the data, returning two 6x3 \n",
    "# (6 levels of gamma x 3 fits per level) arrays of the scores for the training and test sets.\n",
    "\n",
    "#Find the mean score across the three models for each level of gamma for both arrays, creating two arrays of length 6, \n",
    "# and return a tuple with the two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_six():\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import validation_curve\n",
    "    \n",
    "    param_range = np.logspace(-4,1,6)\n",
    "    \n",
    "    train_scores, test_scores = validation_curve(SVC(), X_subset , y_subset,\n",
    "                                                      param_name='gamma',\n",
    "                                                      param_range=param_range, cv=3)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    \n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    \n",
    "    return ( train_scores_mean,  test_scores_mean)\n",
    "    \n",
    "print(answer_six())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 7\n",
    "# Based on the scores from question 6, what gamma value corresponds to a model that is underfitting (and has the worst test \n",
    "# set accuracy)? What gamma value corresponds to a model that is overfitting (and has the worst test set accuracy)? \n",
    "# What choice of gamma would be the best choice for a model with good generalization performance on this dataset \n",
    "# (high accuracy on both training and test set)?\n",
    "\n",
    "# Hint: Try plotting the scores from question 6 to visualize the relationship between gamma and accuracy. Remember to comment \n",
    "# out the import matplotlib line before submission.\n",
    "\n",
    "# This function should return one tuple with the degree values in this order: (Underfitting, Overfitting, Good_Generalization)\n",
    "# Please note there is only one correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_seven():\n",
    "    \n",
    "    df_new=pd.DataFrame({'train_scores_mean':answer_six()[0],'test_scores_mean':answer_six()[1],'gamma_value':np.logspace(-4,1,6)})\n",
    "    \n",
    "    df_new['score_diff']=df_new['train_scores_mean']-df_new['test_scores_mean']\n",
    "    \n",
    "    df_new.set_index('gamma_value',inplace=True)\n",
    "    \n",
    "    df_new.sort_values(by=['score_diff'])\n",
    "    \n",
    "    Good_Generalization=df_new.index[0]\n",
    "\n",
    "    df_new.sort_values(by=['train_scores_mean'])\n",
    "    \n",
    "    Overfitting=df_new.index[-1]\n",
    "\n",
    "    df_new.sort_values(by=['test_scores_mean'])\n",
    "    \n",
    "    Underfitting=df_new.index[0]\n",
    "    \n",
    "    return (Underfitting,Overfitting,Good_Generalization)\n",
    "\n",
    "print(answer_seven())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
